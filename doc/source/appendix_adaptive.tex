\onecolumn
\section{Analysis of \GLUCB}
\label{app:adaptive}
% \setcounter{algorithm}{0}
% We present \GLUCB---a fully sequential adaptive algorithm for solving \QFK.
% Algorithm~\ref{alg:glucb} describes \GLUCB.
% \GLUCB is motivated by \LUCB1~\cite{bib:lucb}. However, it differs from
% \LUCB in subtle manner, due to the very definition of the problem. Like \QF, \QFK
% assumes multiple solutions if $k < m$. On the other hand, it differs from \QF as it
% can solve \SUBSET of size $m > 1$ for $k=m$.

% At each round $t$, we partition $\A$ into three subsets. We keep the $m$ arms
% with the highest empirical average in $A_1^t$, the worst $n-m$ arms in $A_3^t$,
% and the rest in $A_2^t$. Ties are broken arbitrarily. At each round we choose
% a \emph{contentious} arm from each of these three camps. From $A_1^t$ we choose $h_*^t$,
% the arm with the lowest lower confidence bound in $A_1^t$; from $A_2^t$ the arm which is least pulled is chosen, and call it $m_*^t$; from $A_3^t$ we choose $l_*^t$, the arm with the highest
% upper confidence bound. The algorithm stops as soon as the difference between the lower 
% confidence bound of $h_*^t$, and the upper confidence bound of $l_*^t$ become closer than 
% the tolerance. Recently, \citet{Jamieson+N:2014} has shown that using a 
% tailored upper bound, \LUCB can be shown to incur an expected sample complexity
% which is within a $O(\log n)$ factor of the lower bound. Similar technique can
% also be adopted here to make a tighter analysis. However, in the interest of
% keeping the proof simple, we keep our analysis restricted in the conventional approach,
% and leave th tighter analysis as an interesting future exercise. In practice, one can use
% any tighter confidence bound calculation of her choice (as we use KL-divergence based
% confidence bound in our experiments).



% Assume $\mathcal{A} = \{1,2,\cdots,n\}$, is a set of $n$ arms, and the arm $i$ has mean $\mu_i$.
% Also let  indices of the arms are in descending order of their means, \ie $\mu_i \geq \mu_j$
% iff $i \leq j$.
% Let $B_1, B_2, B_3$ are three subsets of $\mathcal{A}$, such that $B_1 \defeq \{1, 2,\cdots, k\}$,
% $B_2 = \{k+1, k+2,\cdots, m\}$ and $B_3=\{m+1, m+2,\cdots, n\}$. For any two arms $a, b \in \mathcal{A}$ we define
% $\Delta_{ab} \defeq \mu_a - \mu_b$. For convenience we slightly overload this notation as
% \begin{equation}\label{eq:defdelta}
%  \Delta_a = \begin{cases}
%   \mu_a - \mu_{m+1} \text{if} a \in B_1\\
%   \mu_k - \mu_{m+1} \text{if} a \in B_2\\
%   \mu_m - \mu_a \text{if} a \in B_3.
%  \end{cases}
% \end{equation}
% We note that $\Delta_k = \Delta_{k+1} = \cdots = \Delta_m = \Delta_{m+1}$.
% Let $u^*(a,t) \defeq \bceil{\frac{32}{\max\{\Delta_a, \frac{\epsilon}{2}\}^2}\ln\frac{k n t^4}{\delta}}$
% for all $a \in \mathcal{A}$. 
% Now, we define the hardness term as
% \begin{equation}
% \label{eq:hardness}
% H_\epsilon = \sum_{a \in \mathcal{A}}\frac{1}{\max\{\Delta_a, \epsilon/2\}^2}.
% \end{equation}


Let at time $t$, $\hatp_a^t$ be the empirical mean of the arm $a \in \mathcal{A}$,
and $u_a^t$ be the number of times the arm $a$ has been pulled
until (and excluding) time $t$. For a given $\delta \in (0,1]$, we define
$\beta(u_a^t, t, \delta) = \sqrt{\frac{1}{2u_a^t}\ln\frac{k_1 n t^4}{\delta}}$, where $k_1=5/4$.
We define upper and lower confidence bound on the estimate of the true
mean of arm $a \in \mathcal{A}$ as $ucb(a,t) = \hatp_a + \beta(u_a^t, t, \delta)$,
and $lcb(a,t) = \hatp_a - \beta(u_a^t, t, \delta)$ respectively.


% \begin{algorithm}[]
% \label{alg:glucb}
% \caption{\textsc{GLUCB}:$(\epsilon, k, m)$-optimal subset selection}
% % \dontprintsemicolon % Some LaTeX compilers require you to use
% % instead
%  \KwIn{$\mathcal{A}$ (\st $|\mathcal{A}| = n$), $k, m, \epsilon, \delta$}
%  \KwOut{$(\epsilon, k, m)$-optimal subset of $\mathcal{A}$}
%  Pull each arm $a  \in \mathcal{A}$ for once. Set $t = n$
%  \Do{$ ucb({l_*^t}, t+1) - lcb({h_*^t}, t+1) > \epsilon $} { \label{ln:stpkoutofm}
% 	 $t = t + 1$
% 	 $A_1^t = \{a : a' \in \mathcal{A}, \hatp_a = \hatp_{a'}\}$ \st $|A_1^t| = k$
% 	 $A_3^t = \{a : a' \in \mathcal{A}, \hatp_a = \hatp_{a'}\}$ \st $|A_3^t| = n-m$
% 	 $A_2^t = \{\mathcal{A} \setminus (A_1^t \cup A_3^t)\}$
% 	 $h_*^t = \arg \max_{\{a \in A_1^t\}} lcb(a,t)$
% 	 $m_*^t = \arg \min_{\{a \in A_2^t\}} u_a^{t}$
% 	 $l_*^t = \arg \max_{\{a \in A_3^t\}} ucb(a,t)$
% 	 pull  $h_*^t,  m_*^t, l_*^t$
%  }
%  \Return $A_1^t$
% \end{algorithm}

% %%%%%%%%% Expected Sample Complexity of \GLUCB %%%%
% \thmscglucb*
% %%%%%%%%%%%%%%
To analyse the sample complexity, first we define some events, at least
one of which must occur if the algorithm does not stop at the round $t$.

\begin{definition}{(\textsc{Probable Events})}
Let $a, b \in \mathcal{A}$, such that $\mu_a > \mu_b$. During the
run of the algorithm, any of the following five events may occur:\\
i) The empirical mean of an arm may falls outside the upper or the lower
confidence bound. We define it as:
$$CROSS_a^t \defeq \{ucb(a,t) < \mu_a \vee lcb(a,t) > \mu_a\}.$$

ii) The empirical mean of arm $a$ may be lesser than that of arm $b$; we definite as:
$$ErrA(a,b,t) \defeq \{\hatp_a^t < \hatp_b^t\}.$$

iii) The lower and upper confidence bounds of arm $a$ may fall below those of arm $b$; we
define them as:
\begin{align*}
 & ErrL(a,b,t) \defeq \{lcb(a,t) < lcb(b,t)\},\\
 & ErrU(a,b,t) \defeq \{ucb(a,t) < ucb(b,t)\}.
\end{align*}

iv) If an arm's confidence bounds are above a certain radius (say $d$), we define that event as
\begin{equation*}
 NEEDY_a^t(d) \defeq \left\{\{lcb(a,t) < \mu_a -d\} \vee \{ucb(a,t) > \mu_a +d\}\right\}.
\end{equation*}
\end{definition}

We show that any arm $a$, if sampled sufficiently, that is $u_a^t \geq u^*(a,t)$, 
then occurrence of any of the \textsc{Probable Events} imply occurrence of $CROSS_a^t$.
First we show that if  $CROSS_a^t$ does not occur for any $a \in \A$, then occurrence
of any one of the \textsc{Probable Events} implies the occurrence of $NEEDY_a^t(\cdot)$
or $NEEDY_b^t(\cdot)$.


%%%%%%% Reducing Events To $NEEDY_a^t$ %%%%%%%%
\begin{restatable}{lemma}{lemErrALUN}[Expressing \textsc{Probable Events} in terms of $NEEDY_a^t$ and $CROSS_a^t$]
\label{lem:ErrALUN}
To prove that $\{\neg  CROSS_a^t \wedge \neg CROSS_b^t\} \wedge \{ErrA(a,b,t) \vee ErrU(a,b,t) \vee ErrL(a,b,t)\} \implies \{NEEDY_a^t\left(\frac{\Delta_{ab}}{2}\right) \vee NEEDY_b^t\left(\frac{\Delta_{ab}}{2}\right) \}$.
\end{restatable}
%%%%%%%%%%%%%%
\begin{proof}
$\mathbf{ErrA(a,b,t)}$: To prove that $\neg \{CROSS_a^t \vee CROSS_b^t\} \wedge ErrA(a,b,t) \implies  NEEDY_a^t\left(\frac{\Delta_{ab}}{2}\right) \vee NEEDY_b^t\left(\frac{\Delta_{ab}}{2}\right)$.
      \begin{align*}
      & ErrA(a,b,t) \implies \hat{p}_a^t < \hat{p}_b^t \\
      & \implies \hat{p}_a^t - (p_a - \beta(u_a^t,t,\delta)) < \hat{p}_b^t - (p_b + \beta(u_b^t,t,\delta) +\\
      & (\beta(u_a^t,t,\delta) + \beta(u_b^t,t,\delta)) - \Delta_{ab}/2)\\
      &\implies  NEEDY_a^t\left(\frac{\Delta_{ab}}{2}\right) \vee NEEDY_b^t\left(\frac{\Delta_{ab}}{2}\right). 
      \end{align*}
    

$\mathbf{ErrU(a,b,t)}$: To prove that $\neg \{CROSS_a^t \vee CROSS_b^t\} \wedge ErrU(a,b,t) \implies NEEDY_b^t\left(\frac{\Delta_{ab}}{2}\right)$.\\
Assuming $\neg CROSS_a^t \wedge \neg CROSS_b^t$ we get
\begin{align*}
& ErrU(a,b,t) \implies \{ucb(b,t) > ucb(a,t)\}\\
& \implies \{\hat{p}_b^t + \beta(u_b^t,t,\delta)> \hat{p}_a^t + \beta(u_a^t,t,\delta)\}\\
& \implies \{\hat{p}_b^t > \mu_b + \beta(u_b^t,t,\delta)\} \vee \{\hat{p}_a^t < \mu_a - \beta(u_a^t,t,\delta)\}  \vee\\ 
& \hspace{28pt} \{2\beta(u_b^t,t,\delta) > \Delta_{ab}\}\\
& \implies NEEDY_b^t\left(\frac{\Delta_{ab}}{2}\right).
\end{align*}
    

$\mathbf{ErrL(a,b,t)}$: To prove that $\neg \{CROSS_a^t \vee CROSS_b^t\} \wedge ErrL(a,b,t) \implies NEEDY_a^t\left(\frac{\Delta_{ab}}{2}\right)$.\\
Assuming $\neg CROSS_a^t \wedge \neg CROSS_b^t$ we get
\begin{align*}
& ErrL(a,b,t) \implies \{lcb(b,t) > lcb(a,t)\}\\
& \implies \{\hat{p}_b^t - \beta(u_b^t,t,\delta)> \hat{p}_a^t - \beta(u_a^t,t,\delta)\}\\
& \implies \{\hat{p}_b^t > \mu_b + \beta(u_b^t,t,\delta)\} \vee \{\hat{p}_a^t < \mu_a - \beta(u_a^t,t,\delta)\}  \vee\\ 
& \hspace{28pt} \{2\beta(u_a^t,t,\delta) > \Delta_{ab}\}\\
& \implies NEEDY_a^t\left(\frac{\Delta_{ab}}{2}\right).
\end{align*}
\end{proof}
We show that given a threshold $d$, if an arm $a$ is sufficiently sampled, such that $\beta(u_a^t, t, \delta) \leq \frac{d}{2}$, then $NEEDY_a^t$ infers $CROSS_a^t$.


%%%%%% NEEDY TO CROSS %%%%%%%%%%%
\begin{restatable}{lemma}{lemneedycross}
 \label{lem:needycross}
  For any $a \in \A$, $\{NEEDY_a^t(d)|\beta(u_a^t, t, \delta) < d/2\} \implies CROSS_a^t$.
\end{restatable}
%%%%%%%%%%%%%%%%

\begin{proof}
 First, we show that $\{lcb(a,t) < \mu_a -d | \beta(u_a^t, t, \delta) < d/2\} \implies CROSS_a^t$,
 \begin{align}
  & \{lcb(a,t) < \mu_a -d | \beta(u_a^t, t, \delta) < d/2\} \nonumber\\
  & \implies \{\hatp_a^t - \beta(u_a^t, t, \delta) < \mu_a -d | \beta(u_a^t, t, \delta) < d/2\} \nonumber\\
  & \implies \{\hatp_a^t < \mu_a -d +\beta(u_a^t, t, \delta) | \beta(u_a^t, t, \delta) < d/2\} \nonumber\\
  & \implies \{\hatp_a^t < \mu_a -d/2 | \beta(u_a^t, t, \delta) < d/2\} \nonumber\\
  & \implies CROSS_a^t.
 \end{align}
 The other part follows the similar way.
\end{proof}

By the very definition of confidence bound, at any round $t$, the probability that
the empirical mean of an arm will lie outside it, is very low. In other words, the
probability of occurrence $CROSS_a^t$ is very low for all $t$ and $a \in \A$.

%%%%%%%%%% Upper bounding the probability of $CROSS_a^t$ %%%%%%%%%%%%%%%%%
\begin{restatable}{lemma}{lemcross}[Upper bounding the probability of $CROSS_a^t$]
 \label{lem:cross}
 $\forall a \in \mathcal{A}$ and $\forall t \geq 0$, $\Pr\{{CROSS_a^t}\}  \leq  \frac{\delta}{knt^4}$. Hence,
 $P\left[\exists t \geq 0  \wedge \exists a \in \mathcal{A} : {CROSS_a^t} | u_a^t \geq 0  \right] \leq  \frac{\delta}{k_1 t^3}.$
\end{restatable}
%%%%%%%%%%%%
\begin{proof}
$\Pr\{{CROSS_a^t}\}$ is upper bounded by using Hoeffding's inequality, and the next statement
gets proved by taking union bound over all arms and $t$.
\end{proof}
Now, recalling the definition of $h_*^t$, and $l_*^t$ from Algorithm~\ref{alg:glucb},
we present the key logic underlying the analysis of \GLUCB. The idea is to show that
if the algorithm has not stopped, then one of those \textsc{Probable Events} must have
occurred. Then using Lemma~\ref{lem:ErrALUN},
 and Lemma~\ref{lem:needycross}, Lemma~\ref{lem:cross}
we show that beyond a certain number of rounds, the probability that \GLUCB
will continue is sufficiently small.
Lastly, using the argument based on pigeon-hole principle, similar to
Lemma 5 of 
\citet{bib:shivaramphdthesis}, we establish the upper bound on the 
sample complexity. Below we present the core logic that shows, until the algorithm stops one of the
\textsc{Probable Events} must occur.


%%%%%%%%%%%%%%%%%%%%%%
%      Case 1
%%%%%%%%%%%%%%%%%%%%%%
\setcounter{logicase}{0}
\begin{logicase}[H]
\begin{algorithmic}
  \IF {$\exists b_3 \in A_1^t \cap B_3$}{
    \STATE Then $ErrL(h_*^t, b_3, t)$ has occurred.
  }\ELSE {
    \STATE $\exists b_3 \in A_2^t \cap B_3$
    \STATE Then $ErrA(h_*^t, b_3, t)$  has occurred.
%     Then $CROSS_{h_*^t}^t \vee CROSS_{b_3}^t$ has occurred
  }\ENDIF
\end{algorithmic}
\label{case:1}
\caption{$h_*^t \in B_1 \wedge l_*^t \in B_1$}
\end{logicase}

%%%%%%%%%%%%%%%%%%%%%%
%      Case 2
%%%%%%%%%%%%%%%%%%%%%%
\begin{logicase}[H]
\begin{algorithmic}
  \IF {$\exists b_3 \in A_1^t \cap B_3$}{
      \STATE Then $ErrL(h_*^t, b_3^t, t)$ has occurred.
  }\ELSE {
    \STATE $\exists b_3 \in A_2^t \cap B_3$.
    \IF {$\Delta_{h_*^t l_*^t} \geq \frac{\Delta_{h_*^t}}{2}$} {
      \STATE Then $NEEDY_{h_*^t}^t(\Delta_{h_*^t}/4) \vee NEEDY_{l_*^t}^t(\Delta_{h_*^t}/4)$ has occurred.
    }\ELSE  {
      \STATE Then $ErrL(l_*^t, b_3^t, t)$ has occurred.
    }\ENDIF
  }\ENDIF
\label{case:2}
\caption{$h_*^t \in B_1 \wedge l_*^t \in B_2$}
\end{algorithmic}
\end{logicase}

%%%%%%%%%%%%%%%%%%%%%%
%      Case 3
%%%%%%%%%%%%%%%%%%%%%%
\begin{logicase}[H]
\begin{algorithmic}
   \STATE Then $NEEDY_{h_*^t}^t(\Delta_{h_*^t}/4) \vee NEEDY_{l_*^t}^t(\Delta_{l_*^t}/4)$ has occurred.
\end{algorithmic}
\label{case:3}
\caption{$h_*^t \in B_1 \wedge l_*^t \in B_3$}
\end{logicase}


%%%%%%%%%%%%%%%%%%%%%%
%      Case 4
%%%%%%%%%%%%%%%%%%%%%%
\begin{logicase}[H]
\begin{algorithmic}
  \IF {$\Delta_{h_*^t l_*^t} \geq \frac{\Delta_{h_*^t}}{2}$}{
    \STATE Then $ErrA(l_*^t, h_*^t, t)$  has occurred.
%     Then $CROSS_{h_*^t}^t \vee CROSS_{l_*^t}^t$ has occurred
  }\ELSE {
    \IF {$\exists b_3 \in A_1^t \cap B_3$}{
      \STATE Then $ErrL(h_*^t, b_3^t, t)$ has occurred.
    }\ELSE {
      \STATE $\exists b_3 \in A_2^t \cap B_3$
      \STATE $\therefore ErrA(l_*^t, b_3, t)$  has occurred.
%       $\therefore CROSS_{l_*^t}^t \vee CROSS_{b_3}^t$ has occurred.
    }\ENDIF
  }\ENDIF
\end{algorithmic}
\label{case:4}
\caption{$h_*^t \in B_2 \wedge l_*^t \in B_1$}
\end{logicase}

%%%%%%%%%%%%%%%%%%%%%%
%      Case 5.A
%%%%%%%%%%%%%%%%%%%%%%

\begin{logicase}[H]
\begin{algorithmic}
 \STATE Here, $\exists b_1 \in (A_2^t \cup A_3^t)\cap B_1$ and $\exists b_3 \in (A_1^t \cup A_2^t)\cap B_3$
  \IF {$|\Delta_{h_*^t l_*^t}| < \Delta_{h_*^t}/2$}{
    \IF {$\Delta_{b_1 h_*^t} > \Delta_{b_1}/4$}{
      \IF {$b_1 \in A_2^t )\cap B_1$}{
	\STATE $ErrA(b_1, h_*^t, t)$
      }\ELSE {
	\STATE $b_1 \in A_3^t \cap B_1$
	\STATE $ErrU(b_1, l_*^t, t)$ has occurred.
      }\ENDIF
    }\ELSE {
      \STATE $\Delta_{b_1 h_*^t} \leq \Delta_{b_1}/4$  and hence $\Delta_{l_*^t b_3} \geq \Delta_{l_*^t}/4$
      \IF {$b_3 \in A_2^t \cap B_3$}{
	\STATE $ErrA(l_*^t, b_3, t)$ has occurred.  
      }\ELSE {
	\STATE $b_3 \in A_1^t \cap B_3$
	\STATE $ErrL(h_*^t, b_3, t)$ has occurred.
      }\ENDIF
    }\ENDIF
  }\ELSE {
    \STATE $|\Delta_{h_*^t l_*^t}| > \Delta_{h_*^t}/2$
    \STATE $NEEDY_{h_*^t}^t (\Delta_{h_*^t}/4) \vee NEEDY_{l_*^t}^t (\Delta_{h_*^t}/4)$ has occurred.
  }\ENDIF
\end{algorithmic}
 \label{case:5.A}
\caption{$h_*^t \in B_2 \wedge l_*^t \in B_2$ and $\Delta_{h_*^t l_*^t} > 0$}
\end{logicase}


%%%%%%%%%%%%%%%%%%%%%%
%      Case 5.B
%%%%%%%%%%%%%%%%%%%%%%
\setcounter{logicase}{4}
\begin{logicase}[H]
\begin{algorithmic}
  \STATE Here, $\exists b_1 \in (A_2^t \cup A_3^t)\cap B_1$ and $\exists b_3 \in (A_1^t \cup A_2^t)\cap B_3$
  \IF {$|\Delta_{h_*^t l_*^t}| < \Delta_{h_*^t}/2$}{
    \IF {$\Delta_{b_1 l_*^t} > \Delta_{b_1}/4$}{
      \IF {$b_1 \in A_2^t \cap B_1$}{
	\STATE $ErrA(b_1, h_*^t, t)$ has occurred.
      }\ELSE {
	\STATE $b_1 \in A_3^t \cap B_1$
	\STATE $ErrU(b_1, l_*^t, t)$ has occurred.
      }\ENDIF
    }\ELSE {
      \STATE $\Delta_{b_1 l_*^t} \leq \Delta_{b_1}/4$ and hence $\Delta_{h_*^t b_3} \geq \Delta_{h_*^t}/4$
      \IF {$b_3 \in A_2^t \cap B_3$}{
	\STATE $ErrA(l_*^t, b_3, t)$ has occurred.  
      }\ELSE {
	\STATE $b_3 \in A_1^t \cap B_3$
	\STATE $ErrL(h_*^t, b_3, t)$ has occurred.
      }\ENDIF
    }\ENDIF
  }\ELSE {
    \STATE $|\Delta_{h_*^t l_*^t}| > \Delta_{h_*^t}/2$
    \STATE $NEEDY_{h_*^t}^t (\Delta_{h_*^t}/4) \vee NEEDY_{l_*^t}^t (\Delta_{h_*^t}/4)$ has occurred.
  }\ENDIF
\end{algorithmic}
  \label{case:5.B}
\caption{(continued) $h_*^t \in B_2 \wedge l_*^t \in B_2$ and $\Delta_{h_*^t l_*^t} \leq 0$} %. Similar to~\ref{case:5.A}
\end{logicase}

%%%%%%%%%%%%%%%%%%%%%%
%      Case 6
%%%%%%%%%%%%%%%%%%%%%%
\begin{logicase}[H]
\begin{algorithmic}
  \IF {$\Delta_{h_*^t l_*^t} \geq \frac{\Delta_{l_*^t}}{2}$}{
    \STATE Then $NEEDY_{h_*^t}^t(\Delta/4) \vee NEEDY_{l_*^t}^t(\Delta_{l_*^t}/4)$ has occurred.
  }\ELSE {
    \STATE $\Delta_{h_*^t l_*^t} < \frac{\Delta_{l_*^t}}{2}$
    \STATE $\therefore \forall b_1 \in \{A_2^t \cup A_3^t\} \cap B_1$, $\Delta_{b_1 h_*^t} > \frac{\Delta_{b_1}}{2}$.
    \IF {$\exists b_1 \in A_2^t \cap B_1$}{
      \STATE $ErrA(b_1, h_*^t, t)$ has occurred.
%       $\therefore CROSS_{h_*^t}^t \vee CROSS_{b_1}^t$ has occurred.
    }\ELSE {
      \STATE $\exists b_1 \in A_3^t \cap B_1$.
      \STATE Then $ErrU(b_1^t, l_*^t,t)$ has occurred.
%       OR
%       $CROSS_{l_*^t}^t \vee CROSS_{b_1}^t$ has occurred.
    }\ENDIF
  }\ENDIF
\end{algorithmic}
\label{case:6}
\caption{$h_*^t \in B_2 \wedge l_*^t \in B_3$}
\end{logicase}

%%%%%%%%%%%%%%%%%%%%%%
%      Case 7
%%%%%%%%%%%%%%%%%%%%%%
\begin{logicase}[H]
\begin{algorithmic}
  \STATE $\therefore ErrA(l_*^t, h_*^t, t)$ has occurred.
%   $CROSS_{h_*^t}^t \vee CROSS_{l_*^t}^t$ has occurred.
\end{algorithmic}
\label{case:7}
\caption{$h_*^t \in B_3 \wedge l_*^t \in B_1$}
\end{logicase}


%%%%%%%%%%%%%%%%%%%%%%
%      Case 8
%%%%%%%%%%%%%%%%%%%%%%

\begin{logicase}[H]
\begin{algorithmic}
  \IF {$\Delta_{h_*^t l_*^t} \geq \frac{\Delta_{h_*^t}}{2}$}{
    \STATE $ErrA(l_*^t, h_*^t, t)$ has occurred.
%     Then $CROSS_{h_*^t}^t \vee CROSS_{l_*^t}^t$ has occurred.
  }\ELSE {
    \STATE $\Delta_{h_*^t l_*^t} < \frac{\Delta_{h_*^t}}{2}$
    \STATE $\therefore \forall b_1 \in \{A_2^t \cup A_3^t\} \cap B_1$, $\Delta_{b_1 l_*^t} > \frac{\Delta_{b_1}}{2}$.
    \IF {$\exists b_1 \in A_2^t \cap B_1$}{
      \STATE $ErrA(b_1, h_*^t, t)$ has occurred.
%       $\therefore CROSS_{h_*^t}^t \vee CROSS_{b_1}^t$ has occurred.
    }\ELSE {
      \STATE $\exists b_1 \in A_3^t \cap B_1$.
      \STATE $\therefore ErrU(b_1, l_*^t, t)$ has occurred.
    }\ENDIF
  }\ENDIF
\end{algorithmic}
\label{case:8}
\caption{$h_*^t \in B_3 \wedge l_*^t \in B_2$}
\end{logicase}

%%%%%%%%%%%%%%%%%%%%%%
%      Case 9
%%%%%%%%%%%%%%%%%%%%%%
\begin{logicase}[H]
\begin{algorithmic}
  \STATE $\exists b_1 \in \{A_2^t \cup A_3^t\} \cap B_1$ %, $\Delta_{1l_*^t} > \frac{\Delta}{2}$.
  \IF {$\exists b_1 \in A_2^t \cap B_1$} {
    \STATE $ErrA(b_1, h_*^t, t)$ has occurred.
  }\ELSE {
    \STATE $\exists b_1 \in A_3^t \cap B_1$
    \STATE $\therefore ErrA(b_1, l_*^t, t)$ has occurred.
  }\ENDIF
%   $\therefore {b_1}^t \vee CROSS_{h_*^t}^t \vee CROSS_{h_*^t}^t$ has occurred.
\end{algorithmic}
\label{case:9}
\caption{$h_*^t \in B_3 \wedge l_*^t \in B_3$}
\end{logicase}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \corgenubscglucb*
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{lemma}[H]
\label{lem:valT}
 If $T = C H_\epsilon\ln\left(\frac{H_\epsilon}{\delta}\right)$, then
 for $C \geq 2732$, the following holds: $$T > 2 + 2 \sum\limits_{a \in \mathcal{A}} u^*(a,T).$$
\end{lemma}
\begin{proof}
This proof is taken from Appendix B.3 of \citet{bib:shivaramphdthesis}.
\begingroup
\allowdisplaybreaks
\begin{align*}
     &2 + 2 \sum_{a \in \mathcal{A}} u^*(a,T) =  2 + 64 \sum_{a \in \mathcal{A}} \bceil{\frac{1}{\max(\Delta_{a}, (\epsilon / 2))^2} \ln \frac{ knt^4}{\delta}}\\
    & \leq  2 + 64n + 64 H_\epsilon\ln \frac{ knT^4}{\delta} \\
    & =  2 + 64n + 64H_\epsilon\ln k + 64H_\epsilon \ln\frac{n}{\delta} + 256 H_\epsilon \ln T\\
    & <  (66 + 64\ln k )H_\epsilon + 64H_\epsilon\ln\frac{n}{\delta}  + 256 H_\epsilon \left[\ln C + \ln H_\epsilon + \ln\ln\frac{H_\epsilon}{\delta}\right]\\
    & <  (66 + 64\ln k )H_\epsilon + 64H_\epsilon\ln\frac{n}{\delta} + 256 H_\epsilon \left[\ln C + \ln H_\epsilon + \ln\ln\frac{H_\epsilon}{\delta}\right]\\
    & <  130H_\epsilon + 64H_\epsilon\ln\frac{n}{\delta} + 256 H_\epsilon \left[\ln C + \ln H_\epsilon + \ln\frac{H_\epsilon}{\delta}\right]\\
    & <  130 H_\epsilon + 64H_\epsilon\ln\frac{H_\epsilon}{\delta}  + 256 H_\epsilon \left[\ln C + 2\ln\frac{H_\epsilon}{\delta}\right]\\
    & < (706 + 256\ln C) H_\epsilon\ln\frac{H_\epsilon}{\delta} <  C H_\epsilon\ln\frac{H_\epsilon}{\delta}\phantom{=}\,\left[\text{For }  C \geq 2732\right].
\end{align*}
\endgroup
\end{proof}

\begin{lemma}\label{lem:t1star}
 Let $T^* = \bceil{2732H_\epsilon\ln\left(\frac{H_\epsilon}{\delta}\right)}$.
 For every $T > T_1^*$, the probability that the Algorithm~\ref{alg:glucb}
 has not terminated after $T$ rounds of sampling is at most $\frac{8\delta}{T^2}$.
\end{lemma}
\begin{proof}
 Letting $\bar{T} = \frac{T}{2}$ we define two events for $ \bar{T} \leq t \leq T-1$:
   $E^{(1)} \defeq \exists a \in \mathcal{A} : {CROSS_a^t}$ and $E^{(2)} \defeq \exists  NEEDY_{a}^t\left(\frac{\Delta_a}{4}\right)$.
  If the algorithm stops for $t<\bar{T}$, then there is nothing to prove. On the contrary, let the algorithm has not stopped after
  $t > \bar{T}$ and neither $E^{(1)}$ nor $E^{(2)}$ has occurred. 
  Letting $N_{rounds}$ be the the  required number of rounds beyond $\bar{T}$,
  we can upper bound it as:
 \begin{align*}
  &N_{rounds} = \sum\limits_{t=\bar{T}} \left\{\idop\left[NEEDY_{h_*^t}^t\left(\frac{\Delta_{h_*^t}}{4}\right) \vee NEEDY_{m_*^t}^t\left(\frac{\Delta_{m_*^t}}{4}\right)  \vee   NEEDY_{l_*^t}^t\left(\frac{\Delta_{l_*^t}}{4}\right)\right]\right\}\\
  & \leq  \sum_{\bar{T}}^{T-1} \sum_{a \in \mathcal{A}}\idop\left[a \in \{h_*^t, m_*^t,  l_*^t\}\wedge NEEDY_{a}^t\left(\frac{\Delta_a}{4}\right)\right]\\
  & =  \sum_{\bar{T}}^{T-1} \sum_{a \in \mathcal{A}}\idop[a \in \{h_*^t, m_*^t, l_*^t\}\wedge(u_a^t < u^*(a,t))]\\
  & \leq  \sum_{\bar{T}}^{T-1} \sum_{a \in \mathcal{A}}\idop[a \in \{h_*^t, m_*^t, l_*^t\} \wedge(u_a^t < u^*(a,t))]\\
  & \leq  \sum_{a \in \mathcal{A}} \sum_{\bar{T}}^{T-1}\idop[(a \in \{h_*^t, m_*^t, l_*^t\})\wedge(u_a^t < u^*(a,t))]\\
  & \leq  \sum_{a \in \mathcal{A}}u^*(a,t).
 \end{align*}
  Using Lemma~\ref{lem:valT}, $T \geq T^* \Rightarrow T > 2 + 2\sum_{a \in \mathcal{A}}u^*(a,t)$.
 Hence, if neither $E^{(1)}$ nor $E^{(2)}$ occurs then the
 algorithm runs for at most $\bar{T} + N_{rounds} \leq \ceil{T/2} + \sum_{a \in \mathcal{A}}16u^*(a,t) < T$ 
 number of rounds.
  

 The probability that the algorithm does not stop within $T$ rounds, is upper-bounded
 by $P[E^{(1)} \vee E^{(2)}]$. Applying Lemma~\ref{lem:needycross} and Lemma~\ref{lem:cross},
 \begin{align*}
  & P[E^{(1)} \vee E^{(2)}] \leq \sum_{t = \bar{T}}^{T-1} \left(\frac{\delta}{k_1 t^3} + \frac{\delta}{kt^4}\right) \leq \sum_{t = \bar{T}}^{T-1} \frac{\delta}{k_1 t^3}\left(1+\frac{2}{t}\right) \leq  \left(\frac{T}{2}\right)\frac{8\delta}{k_1 T^3}\left(1 + \frac{4}{T}\right) < \frac{8\delta}{T^2}. %[\because n \geq 2]\nonumber
 \end{align*}
\end{proof}

\thmscglucb*
Using Lemma~\ref{lem:valT}, and Lemma~\ref{lem:t1star}
 the expected sample complexity of the Algorithm~\ref{alg:glucb} can be upper bounded as
 \begin{align}
  & E[SC] \leq 2\left(T_1^* + \sum_{t=T_1^*}^\infty \frac{8\delta}{T^2}\right) \leq 5464\cdot\left(H_\epsilon\ln\left(\frac{H_\epsilon}{\delta}\right)\right) + 32.
 \end{align}
