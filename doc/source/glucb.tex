\subsection{An Adaptive Algorithm for Solving \protect\QFK} %\textsc{LUCB}$(k, m)$
\label{sec:adaptive}

% Although there exist instances of \QFK that require within a constant factor of the sample complexity of \GHALVING, the algorithm is likely to be wasteful on ``easy'' instances, in which arms in $\TOPM$ are well-separated from the remaining arms.

%We present an adaptive algorithm, \GLUCB, for solving $(k,m,n)$ and analyse its sample complexity.

Algorithm~\ref{alg:glucb} describes \GLUCB, a fully sequential algorithm, which for $k=1$ has the same
guarantee on sample-complexity as \FF, but empirically appears to be more economical. The algorithm  generalises \LUCB~\cite{bib:lucb}, which solves $(m, m, n)$. 

% However, it differs from
% \LUCB in a subtle manner, due to the very definition of the problem. Like \QF, \QFK
% assumes multiple solutions if $k < m$. On the other hand, it differs from \QF as it
% can solve \SUBSET of size $m \geq 1$ for $k=m$.

% \begin{algorithm}[]
% \small{
% \caption{\GLUCB: Algorithm to select $k$ $(\epsilon, m)$-optimal arms}
% \label{alg:glucb}
% \DontPrintSemicolon% instead
%  \KwIn{$\mathcal{A}$ (\st $|\mathcal{A}| = n$), $k, m, \epsilon, \delta$.}
%  \KwOut{$k$ distinct $(\epsilon,m)$-optimal arms from $\mathcal{A}$.}
%  Pull each arm $a  \in \mathcal{A}$ once. Set $t = n$.\;
%  \Do{$ ucb({l_*^t}, t+1) - lcb({h_*^t}, t+1) > \epsilon.$} { \label{ln:stpkoutofm}
%      $t = t + 1$.\;
% %      $A_1^t = \{a : a' \in \mathcal{A}, \hatp_a = \hatp_{a'}\}$ \st $|A_1^t| = k$.\;
% %      $A_3^t = \{a : a' \in \mathcal{A}, \hatp_a = \hatp_{a'}\}$ \st $|A_3^t| = n-m$.\;
% 	 $A_1^t \defeq $ Set of $k$ arms with the highest empirical means.\;
%      $A_3^t \defeq $ Set of $n-m$ arms with the lowest empirical means.\;
%      $A_2^t \defeq \{\mathcal{A} \setminus (A_1^t \cup A_3^t)\}$.\;
%      $h_*^t = \arg \max_{\{a \in A_1^t\}} lcb(a,t)$.\;
%      $m_*^t = \arg \min_{\{a \in A_2^t\}} u_a^{t}$.\;
%      $l_*^t = \arg \max_{\{a \in A_3^t\}} ucb(a,t)$.\;
%      pull  $h_*^t,  m_*^t, l_*^t$.
%  }
%  \Return $A_1^t$.\;
%  }
% \end{algorithm}

\begin{algorithm}[ht]
\begin{algorithmic}
\small{
 \REQUIRE {$\mathcal{A}$ (\st $|\mathcal{A}| = n$), $k, m, \epsilon, \delta$.}
 \ENSURE {$k$ distinct $(\epsilon,m)$-optimal arms from $\mathcal{A}$.}
 \STATE Pull each arm $a  \in \mathcal{A}$ once. Set $t = n$.
 \WHILE {$ ucb({l_*^t}, t+1) - lcb({h_*^t}, t+1) > \epsilon.$} { \label{ln:stpkoutofm}
     \STATE $t = t + 1$.
     \STATE $A_1^t \defeq $ Set of $k$ arms with the highest empirical means.
     \STATE $A_3^t \defeq $ Set of $n-m$ arms with the lowest empirical means.
     \STATE $A_2^t \defeq \mathcal{A} \setminus (A_1^t \cup A_3^t)$.
     \STATE $h_*^t = \arg \max_{\{a \in A_1^t\}} lcb(a,t)$.
     \STATE $m_*^t = \arg \min_{\{a \in A_2^t\}} u_a^{t}$.
     \STATE $l_*^t = \arg \max_{\{a \in A_3^t\}} ucb(a,t)$.
     \STATE pull  $h_*^t,  m_*^t, l_*^t$.
 }\ENDWHILE
 \STATE Return $A_1^t$.
 }
\end{algorithmic}
\caption{\GLUCB: Algorithm to select $k$ $(\epsilon, m)$-optimal arms}
\label{alg:glucb}
\end{algorithm}


At each round $t$, we partition $\A$ into three subsets. We keep the $k$ arms
with the highest empirical averages in $A_1^t$, the $n-m$ arms with the lowest empirical averages in $A_3^t$,
and the rest in $A_2^t$; ties are broken arbitrarily (uniformly at random in our experiments). At each round we choose
a \emph{contentious} arm from each of these three sets: from 
$A_1^t$ we choose $h_*^t$,
the arm with the lowest lower confidence bound (LCB); from $A_2^t$ the arm which is least pulled is chosen, and called $m_*^t$; from $A_3^t$ we choose $l_*^t$, the arm with the highest
upper confidence bound (UCB). The algorithm stops as soon as the difference between the lower 
confidence bound of $h_*^t$, and the upper confidence bound of $l_*^t$ becomes no larger than 
the tolerance $\epsilon$.

Let $B_1, B_2, B_3$ be corresponding sets based on the true means: that is, subsets of $\mathcal{A}$ such that $B_1 \defeq \{1, 2,\cdots, k\}$,
$B_2 = \{k+1, k+2,\cdots, m\}$ and $B_3=\{m+1, m+2,\cdots, n\}$. For any two arms $a, b \in \mathcal{A}$ we define
$\Delta_{ab} \defeq \mu_a - \mu_b$. For the sake of convenience we slightly overload this notation as
{\footnotesize
\begin{equation}\label{eq:defdelta}
 \Delta_a = \begin{cases}
  \mu_a - \mu_{m+1}\; \text{if}\; a \in B_1\\
  \mu_k - \mu_{m+1}\; \text{if}\; a \in B_2\\
  \mu_m - \mu_a\;\;\;\;\; \text{if}\; a \in B_3.
 \end{cases}
\end{equation}
}
% \begin{table}[H]
% \centering
% \caption{My caption}
% \label{my-label}
% \begin{tabular}{l|lll}
% \hline
%  & $a \in B_1$ & $a \in B_2$ & $a \in B_3$ \\ \hline
% $\Delta_a$ & $\mu_a - \mu_{m+1}$ & $\mu_k - \mu_{m+1}$ & $\mu_m - \mu_a$ \\ \hline
% \end{tabular}
% \end{table}
We note that $\Delta_k = \Delta_{k+1} = \cdots = \Delta_m = \Delta_{m+1}$.
Let $u^*(a,t) \defeq \bceil{\frac{32}{\max\{\Delta_a, \frac{\epsilon}{2}\}^2}\ln\frac{k_1 n t^4}{\delta}}$ for all $a \in \mathcal{A}$, where $k_1=5/4$. 
Now, we define the hardness term as $H_\epsilon = \sum_{a \in \mathcal{A}}\frac{1}{\max\{\Delta_a, \epsilon/2\}^2}$.

\begin{restatable}{theorem}{thmscglucb}[Expected Sample Complexity of \GLUCB]
\label{thm:scglucb}
\GLUCB solves \QFK using an expected sample complexity upper bounded by
$O\left(H_\epsilon \log\frac{H_\epsilon}{\delta}\right)$. 
\end{restatable}
Appendix-A describes the proof in detail. The core argument 
is similar to
that for Algorithm $\F_2$ by \citet{bib:arcsk2017}. However, it subtly differs due to the different strategy for choosing arms since the output set is
not necessarily singleton.
%  Recently, \citet{Jamieson+N:2014} has shown that using a 
% tailored upper bound, \LUCB can be shown to incur an expected sample complexity
% which is within a $O(\log n)$ factor of the lower bound. A similar technique can
% also be adopted here to make a tighter analysis. However, in the interest of
% keeping the proof simple, we keep our analysis restricted in the conventional approach and leave the tighter analysis as a future exercise. 
In practice, one can use
tighter confidence bound calculations (we use KL-divergence based
confidence bounds in our experiments) to get even better sample complexity.
% To analyse the sample complexity, first we define some events, at least
% one of which must occur if the algorithm does not stop at the round $t$.

% \begin{definition}{(\textsc{Probable Events})}
% Let $a, b \in \mathcal{A}$, such that $\mu_a > \mu_b$. During the
% run of the algorithm, any of the following five events may occur:
% $CROSS_a^t \defeq \{ucb(a,t) < \mu_a \vee lcb(a,t) > \mu_a\}$,
% $ErrA(a,b,t)) \defeq \{\hatp_a^t < \hatp_b^t\}$,
% $ErrL(a,b,t) \defeq \{lcb(a,t) < lcb(b,t)\}$,
% $ErrU(a,b,t) \defeq \{ucb(a,t) < ucb(b,t)\}$,
% $ NEEDY_a^t(d) \defeq \{\{lcb(a,t) < \mu_a - d\} \vee \{ucb(a,t) > \mu_a + d\}\}$.
% \end{definition}

% We show that any arm $a$, if sampled sufficiently, that is $u_a^t \geq u^*(a,t)$, 
% then occurrence of any of the \textsc{Probable Events} imply occurrence of $CROSS_a^t$.
% First we show that if  $CROSS_a^t$ does not occur for any $a \in \A$, then occurrence
% of any one of the \textsc{Probable Events} implies the occurrence of $NEEDY_a^t(\cdot)$
% or $NEEDY_b^t(\cdot)$.
% It is important to note that as $m_*^t$ is the least sampled arm in $A_2^t$,
% for any arm $a \in A_2^t$, $NEEDY_a^t \implies NEEDY_{m_*^t}^t$.


% \begin{restatable}{lemma}{lemErrALUN}[Reducing Events To $NEEDY_a^t$]
% \label{lem:ErrALUN}
% To prove that $\{\neg  CROSS_a^t \wedge \neg CROSS_b^t\} \wedge \{ErrA(a,b,t) \vee ErrU(a,b,t) \vee ErrL(a,b,t)\} \implies \{NEEDY_a^t(\frac{\Delta_{ab}}{2}) \vee NEEDY_b^t(\frac{\Delta_{ab}}{2}) \}$.
% \end{restatable}


% We show that given a threshold $d$, if an arm $a$ is sufficiently sample such that $\beta(u_a^t, t, \delta) \leq \frac{d}{2}$, then that $NEEDY_a^t$ infers $CROSS_a^t$.

% \begin{restatable}{lemma}{lemneedycross}
%  \label{lem:needycross}
%   For any $a \in \A$, $\{NEEDY_a^t(d)|\beta(u_a^t, t, \delta) < d/2\} \implies CROSS_a^t$
% \end{restatable}

% By the very definition of confidence bound, at any round $t$, the probability that
% the empirical mean of an arm will lie outside it is very low. In other words, the
% probability that $CROSS_a^t$ occur is very low for all $t$ and $a \in \A$.

% \begin{restatable}{lemma}{lemcross}[Upper bounding the probability of $CROSS_a^t$]
%  \label{lem:cross}
%  $\forall a \in \mathcal{A}$ and $\forall t \geq 0$, $\Pr\{{CROSS_a^t}\}  \leq  \frac{\delta}{knt^4}$. Hence,
%  $P\left[\exists t \geq 0  \wedge \exists a \in \mathcal{A} : {CROSS_a^t} | u_a^t \geq 0  \right] \leq  \frac{\delta}{k_1 t^3}.$
% \end{restatable}
% \begin{proof}
% $\Pr\{{CROSS_a^t}\}$ is upper bounded by using Hoeffding's inequality, and the next statement
% gets proved by taking union bound over all arms and $t$.
% \end{proof}
% Now, recalling the definition of $h_*^t$, and $l_*^t$ from Algorithm~\ref{alg:glucb},
% we present the key logic underlying the analysis of \GLUCB. The idea is to show that
% if the algorithm has not stopped, then one of those \textsc{Probable Events} must have
% occurred. Then using Lemma~\ref{lem:cross}, Lemma~\ref{lem:ErrALUN},
%  and Lemma~\ref{lem:needycross}, 
% we show that beyond a certain number of rounds, the probability that \GLUCB
% will continue is upper bounded is sufficiently small.
% Lastly, using the argument based on pigeon-hole principle, similar to
% \citet[Lemma 5]{bib:shivaramphdthesis}, we establish the upper bound on the 
% sample complexity. The core logic to show that one of the probable events must occur until the algorithm stops is presented below.

% \begin{restatable}{lemma}{lemglucb_cases}
% \label{lem:glucb_cases}
% If at \GLUCB has not stopped at round $t$, then for $a \in \{h_*^t, l_*^t\}$, and $b \in \{h_*^t, l_*^t\} \setminus \{a\}$, one of the \textsc{Probable Events} must have occurred.
% \end{restatable}
% \begin{proof}
% Recalling the definitions of the ground truth sets $B_1, B_2$ and $B_3$,
% as $\{h_*^t, l_*^t\} \in B_1 \cup B_2 \cup B_3$, depending from which sets
% out of these three, $h_*^t$, and $l_*^t$ have come, there are total $3 \times 3 = 9$
% possible cases. 
% The detailed analysis of all the 
% cases is presented in the Appendix~\ref{app:adaptive}.We present the case
% where $h_*^t \in B_2 \wedge l_*^t \in B_2$ in Figure~\ref{fig:logicb2b2}. 
% Any arm in $B_2$ may belong to correct output set, but not mandatory. However, 
% one of the arms in $\{h_*^t, l_*^t\}$ can have mean arbitrarily close to the
% worst arm in $B_1$, while the 
% other one's mean can be arbitrarily close to the best arm in $B_3$.
% On the other hand, as both $h_*^t$, and $l_*^t$
% belong to $B_2$, their means can be arbitrarily close, and hence, even if
% $h_*^t$ and $l_*^t$ are sampled $u^*(h_*^t)$ and $u^*(l_*^t)$ times respectively,
% $ ucb({l_*^t}, t+1) - lcb({h_*^t}, t+1) < \epsilon $ may not hold. 
% However, this will not occur as it will lead some other contradiction. 

% % This special
% % properties of arms in $B_2$ makes this case the most interesting among all.
% % \fbox{
% \begin{figure}
% \framebox[\columnwidth][s]{
% \begin{minipage}[l][][l]{0.45\textwidth}
% \flushleft
%  Suppose $h_*^t \in B_2 \wedge l_*^t \in B_2$ and $\Delta_{h_*^t l_*^t} > 0$.\\
%  Then, $\exists b_1 \in (A_2^t \cup A_3^t)\cap B_1$ and $\exists b_3 \in (A_1^t \cup A_2^t)\cap B_3$.\\
% %   letting $\mu_{h_*^t} > \mu_{l_*^t}$\filler{WRONG!!!}\;
%   \If{$|\Delta_{h_*^t l_*^t}| < \Delta_{h_*^t}/2$}{
%     \If{$\Delta_{b_1 h_*^t} > \Delta_{b_1}/4$}{
%       \If{$b_1 \in A_2^t )\cap B_1$}{
%     $ErrA(b_1, h_*^t, t)$
%       }\Else{
%     $b_1 \in A_3^t \cap B_1$\\
%     $ErrU(b_1, l_*^t, t)$
%       }
%     }\Else{
%       $\Delta_{b_1 h_*^t} \leq \Delta_{b_1}/4$  and hence $\Delta_{l_*^t b_3} \geq \Delta_{l_*^t}/4$\\
%       \If{$b_3 \in A_2^t \cap B_3$}{
%     $ErrA(l_*^t, b_3, t)$  
%       }\Else{
%     $b_3 \in A_1^t \cap B_3$\;
%     $ErrL(h_*^t, b_3, t)$
%       }
%     }
%   }\Else{
%     $|\Delta_{h_*^t l_*^t}| > \Delta_{h_*^t}/2$\\
%     $$NEEDY_{h_*^t}^t (\Delta_{h_*^t}/4) \vee NEEDY_{l_*^t}^t (\Delta_{h_*^t}/4)$$
%   }\par
% \end{minipage}%
% \hspace{2pt}
% \vline
% \hspace{1pt}
% \begin{minipage}[r][][l]{0.45\textwidth}
%  Suppose $h_*^t \in B_2 \wedge l_*^t \in B_2$ and $\Delta_{h_*^t l_*^t} \leq 0$.
%  Then, $\exists b_1 \in (A_2^t \cup A_3^t)\cap B_1$ and $\exists b_3 \in (A_1^t \cup A_2^t)\cap B_3$.\\
% %   letting $\mu_{h_*^t} > \mu_{l_*^t}$\filler{WRONG!!!}\;
%   \If{$|\Delta_{h_*^t l_*^t}| < \Delta_{h_*^t}/2$}{
%     \If{$\Delta_{b_1 l_*^t} > \Delta_{b_1}/4$}{
%       \If{$b_1 \in A_2^t )\cap B_1$}{
%     $ErrA(b_1, h_*^t, t)$
%       }\Else{
%     $b_1 \in A_3^t \cap B_1$\\
%     $ErrU(b_1, l_*^t, t)$
%       }
%     }\Else{
%       $\Delta_{b_1 l_*^t} \leq \Delta_{b_1}/4$ and hence $\Delta_{h_*^t b_3} \geq \Delta_{h_*^t}/4$\\
%       \If{$b_3 \in A_2^t \cap B_3$}{
%     $ErrA(l_*^t, b_3, t)$  
%       }\Else{
%     $b_3 \in A_1^t \cap B_3$\\
%     $ErrL(h_*^t, b_3, t)$
%       }
%     }
%   }\Else{
%     $|\Delta_{h_*^t l_*^t}| > \Delta_{h_*^t}/2$\\
%     $$NEEDY_{h_*^t}^t (\Delta_{h_*^t}/4) \vee NEEDY_{l_*^t}^t (\Delta_{h_*^t}/4)$$
%   }
% \end{minipage}
% }
% \caption{Shows that if \GLUCB does not stop and both $h_*^t$, $l_*^t \in B_2$,
% then one of the \textsc{Probable Events} must have occurred. For the analysis of 
% all the cases vide Appendix~\ref{app:adaptive}.}
% \label{fig:logicb2b2}
% \end{figure}
% \end{proof}

% \begin{restatable}{corollary}{corgenubscglucb}
% \label{cor:gen_ubsc_glucb}
% For any arm  $a \A$, our definition of $\Delta_a$ coincides with the one
% defined by \citet{bib:arcsk2017} for $k=1$, and for $k=m$ it coincides with
% the one by \citet{bib:lucb}. As The hardness term $H_\epsilon$ is the same
% function of $\Delta_a$ and $\epsilon$, the upper bound on the sample complexity of
% \GLUCB matches with those for $\F_2$ and \LUCB for $k=1$, and $k=m$ respectively.
% \end{restatable}

% \GLUCB being an adaptive algorithm the sample complexity is dependent on the relative differences
% of means of the arms. Next, we are going to present a non-adaptive algorithm that solves \QFK
% with a number of samples that matches the lower bound up to a constant factor.