\section{Algorithms for Finite Instances}
\label{sec:finiteinst}

We begin our technical presentation by furnishing a lower bound on the sample complexity of algorithms for \QFK.

\subsection{Lower Bound on the Sample-Complexity}
\label{subsec:lbsckmn}

\begin{restatable}{theorem}{thmlbmainthm}[Lower Bound for \QFK]
\label{thm:lbmainthm}
Let $\mathcal{L}$ be an algorithm that solves \QFK. Then, there exists an 
instance $(\A, n, m, k, \epsilon, \delta)$, 
with $0< \epsilon \leq \frac{1}{\sqrt{32}}$, $0 < \delta \leq \frac{e^{-1}}{4}$, 
and $n \geq 2m$, $1 \leq k \leq m$, on which the expected number of pulls 
performed by $\mathcal{L}$ is at least $\frac{1}{18375}. \frac{1}{\epsilon^2}. \frac{n}{m-k+1}\ln\frac{\binom{m}{k - 1}}{4\delta}$.
\end{restatable}

The detailed proof of the theorem is given in Appendix~\ref{app:lowerboundqfk}. 
The proof generalises lower bound proofs for both $(m, m, n)$
~\citep[see Theorem 8]{bib:lucb} and $(1, m, n)$~\citep[see Theorem 3.3]{bib:arcsk2017}.
The core idea in these proofs is to consider two sets of bandit instances,
$\mathcal{I}$ and $\mathcal{I}^{\prime}$, such that over ``short'' trajectories, 
an instance from $\mathcal{I}$ will yield the same reward sequences as a corresponding 
instance from $\mathcal{I}^{\prime}$, with high probability. Thus, any algorithm 
will return the same set of arms for both instances, with high probability. 
However, by construction, no set of arms can be simultaneously correct for both 
instances---implying that a correct algorithm must encounter sufficiently ``long'' 
trajectories. Our main contribution is in the design of 
$\mathcal{I}$ and $\mathcal{I}^{\prime}$ when $k \in \{1, 2, \dots, m\}$ 
(rather than exactly $1$ or $m$) arms have to be returned.

Our algorithms to achieve improved \textit{upper} bounds for \QF and \QFK 
(across bandit instances) follow directly from methods we design for the 
infinite-armed setting in Section~\ref{sec:infinitemab} (see Corollary~\ref{cor:qffromqptighter} 
and Corollary~\ref{cor:qfkfromqpktighter}). In the remainder of this section, we present a fully-sequential algorithm for \QFK whose expected sample complexity varies with the ``hardness'' of the input instance.

% \input{ghalving.tex}
\input{glucb.tex}
%  We two algorithms that solve \QFK: one with optimal worst-case sample complexity, and the other
Next, we are going to consider infinite-armed bandit instances, and present the algorithms to solve them.