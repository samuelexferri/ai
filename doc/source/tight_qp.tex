\section{Algorithm for Infinite Instances}
\label{sec:infinitemab}

Before proceeding to the identification of $k$ 
$[\epsilon, \rho]$-optimal arms in infinite-armed bandits, we revisit the case of $k = 1$. To find a single $[\epsilon, \rho]$-optimal arm,
the sample complexity of all the existing algorithms~\citep{bib:arcsk2017,Aziz+AKA:2018} scales as
$(1/\rho \epsilon^{2})\log^2(1/\delta)$, for the given mistake probability $\delta$. In this section
we present an algorithm \PPP whose sample complexity is only an \textit{additive} poly-log factor
away from the lower bound of $\Omega((1/\rho \epsilon^{2})\log 1/\delta)$~\citep[Corollary 3.4]{bib:arcsk2017}.
 
\subsection{Solving \protect\QP Instances}
\label{subsec:tighterqp}
\PPP is a two-phase algorithm.
In the first phase, it runs a sufficiently large number of independent copies of \PP and chooses
a large subset of arms  (say of size $u$), in which every arm is $[\epsilon, \rho]$-optimal 
with probability at least $1-\delta'$, where $\delta'$ is some small \textit{constant}. 
The value $u$ is chosen in a manner such that at least one of the chosen arms is 
$[\epsilon/2, \rho]$-optimal with probability at least $\delta/2$.
The second phase solves the best arm identification problem $(1,1,u)$ by applying \textsc{Median Elimination}.

Algorithm~\ref{alg:tightqp1} describes \PPP. It uses \PP~\citep{bib:arcsk2017} with  \textsc{ Median Elimination}  as a 
subroutine, to select an  $[\epsilon, \rho]$-optimal arm with confidence $1-\delta'$.
We have assumed $\delta' = 1/4$, in practice the one can choose any
sufficiently small value for it, which will merely affect the multiplicative constant in the upper bound.
% \begin{algorithm}[]
% \DontPrintSemicolon
% \small{
% \caption{$\mathcal{P}_3$}
%  \KwIn{ $\mathcal{A}, \epsilon, \delta$.}
% \label{alg:tightqp1}
%  Set $\delta' = 1/4$, $u = \bceil{\frac{1}{1-\delta'}\log\frac{2}{\delta}} = \bceil{\frac{4}{3}\log\frac{2}{\delta}}$.\;
%  Run $u$ copies of $\mathcal{P}_2(\A, \rho, \epsilon/2, \delta')$ and form set $S$ with the output arms.\;
%  Identify the $(\epsilon/2,1)$-optimal arm in $S$ using \textsc{Median Elimination} with confidence at least $1-\delta/2$.
% %  Return the output from \GHALVING$\left(S, u, \floor{\frac{u}{2}}, 1, \frac{\epsilon}{2}, \frac{\delta}{2}\right)$.
%  }
%  \end{algorithm}

\begin{algorithm}[ht]
\begin{algorithmic}
\small{
 \REQUIRE { $\mathcal{A}, \epsilon, \delta$.}
 \ENSURE {One $[\epsilon, \rho]$-optimal arm.}
 \STATE Set $\delta' = 1/4$, $u = \bceil{\frac{1}{\delta'}\log\frac{2}{\delta}} = \bceil{4\log\frac{2}{\delta}}$.
 \STATE Run $u$ copies of $\mathcal{P}_2(\A, \rho, \epsilon/2, \delta')$ and form set $S$ with the output arms.
 \STATE Identify an $(\epsilon/2,1)$-optimal arm in $S$ using \textsc{Median Elimination} with confidence at least $1-\delta/2$.
 }
 \end{algorithmic}
 \caption{$\mathcal{P}_3$}
\label{alg:tightqp1}
 \end{algorithm}



 \begin{restatable}{theorem}{thmppp}[Correctness and Sample Complexity of \PPP]
 \label{thm:ppp}
 \PPP  solves \QP, with sample complexity 
%  $O\left(\frac{1}{\epsilon^2}\left(\frac{1}{\rho}\log\frac{1}{\delta} + \log^2\frac{1}{\delta}\right)\right)$. 
$O(\epsilon^{-2}(\rho^{-1}\log(1/\delta) + \log^2(1/\delta)))$.
 \end{restatable}
 \begin{proof}
 First we prove the correctness and then upper-bound the sample complexity.

\paragraph{Correctness.} First we notice that each copy of $\mathcal{P}_2$ outputs an $[\epsilon/2, \rho]$-optimal arm
 with probability at least $1-\delta'$. 
%  Also, \GHALVING outputs an
%  $[\epsilon/2, \rho]$-optimal arm with probability $1-\delta$.
 Now, $S \cap \TOPRHO = \emptyset$ can only happen if all the $u$ copies of \PP output sub-optimal arms. Therefore, $\Pr\{S \cap \TOPRHO = \emptyset\} = (1-\delta')^{u} \leq \delta/2$.
%  Let, $\hat{X}$ be the fraction of sub-optimal arms in $S$. Then $\Pr\{\hat{X} \geq \frac{1}{2}\}$ $= \Pr\{\hat{X} - \delta' \geq \frac{1}{4}\}$
%   $\leq \exp(2\cdot(\frac{1}{4})^2\cdot u) = \exp(-2\cdot\frac{1}{16}\cdot 8\log\frac{2}{\delta}) < \frac{\delta}{2}$. 
  On the other hand, the mistake probability of \textsc{Median Elimination} is upper bounded by $\delta/2$. Therefore, by taking union bound, we get the 
  mistake probability is upper bounded by $\delta$. Also, the mean of the output arm is not
  less than $\frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon$ from the $(1-\rho)$-th
  quantile.
  
  \paragraph{Sample complexity.} First we note that, for some appropriate constant $C$,
  the sample complexity (SC) of each of the $u$ copies of $\mathcal{P}_2$ is $\frac{C}{\rho(\epsilon/2)^2}\left(\ln\frac{2}{\delta'}\right)^2 \in O\left(\frac{1}{\rho\epsilon^2}\right)$.
  Hence, SC of all the $u$ copies $\mathcal{P}_2$ together is upper bounded by $\frac{C_1\cdot u}{\rho\epsilon^2}$, for some constant $C_1$.
  Also, for some constant $C_2$, the sample complexity of \textsc{Median Elimination} is upper bounded by $\frac{C_2\cdot u}{ (\epsilon/2)^2}\ln\frac{2}{\delta} \leq \frac{C_3}{\epsilon^2}\ln^2\frac{2}{\delta}$.
  Adding the sample complexities and substituting for $u$ yields the bound.
 \end{proof}
 
% Although, sample complexity of \PPP matches the lower bound up to a constant factor, in practice its performance is
% not satisfactory due to large value of the constant. 
% For example, the number of samples required to select $u$ $[\epsilon, \rho]$-optimal arms is 
% $\frac{C \cdot u}{\rho (\epsilon/2)^2} \log^2\frac{2}{\delta'} \geq \bceil{\frac{C}{\rho \epsilon^2}\log\frac{2}{\delta} \cdot 4\frac{\log^2(1/\delta')}{0.5-\delta'}} \geq \bceil{\frac{109 C}{\rho \epsilon^2}\log\frac{2}{\delta}}$, minimising with respect to $\delta' $.
% In the second phase a big number of samples add up to this to make it even bigger. 
% Therefore, \PPP can not outperform \PP if $\delta \geq 2^{-109}$, which is effectively  far below
% the practical range.

\begin{corollary}
\label{cor:qffromqptighter}
\PPP can solve any instance of \QF $(\A, n, m, \epsilon, \delta)$ with sample complexity
$O\left(\frac{1}{\epsilon^2}\left(\frac{n}{m}\log\frac{1}{\delta} + \log^2\frac{1}{\delta}\right)\right)$.
\end{corollary}
%\filler{NEED A COUPLE OF LINES DESCRIBING HOW.}
\begin{proof}
Let, $(\A, n, m, \epsilon,\delta)$ be the given instance of \QF.
We partition the set $\A^\infty = [0,1]$ in to $n$ equal segments and associate each 
with a unique arm in $\A$, and such that no two different subsets get associated
with the same arm. Now, defining $P_{A^\infty} = Uniform[0,1]$, and $\rho' = m/n$,
we realise that solving the \QP instance $(\A^\infty, P_{\A^\infty}, \rho', \epsilon,\delta)$
solves the original \QF instance, thereby proving the corollary.
 \end{proof}
% Given an instance of \QF by $(\A, n, m, \epsilon,\delta)$, we can transform it
% to an instance of \QP given by $(\A^\infty, P_{\A^\infty}, \rho', \epsilon,\delta)$, where
% $P_\A$ is an uniform distribution over $\A$, and $\rho = m/n$. Now, solving this \QP
% solves the original \QF, thereby proving Corollary~\ref{cor:qffromqptighter}.

At this point it is of natural interest to find an efficient algorithm to solve \QPK.
Next, we discuss the extension of \QP to \QPK, and present lower and  upper bound 
on the sample complexity needed to solve it.
% this in detail.
% However, as discussed in Section~\ref{sec:problemdefinitionandcontributions},
% problem instances have to be 


\subsection{Solving ``At Most $k$-equiprobable'' \protect\QPK Instances}
\label{subsec:tighterqpk}

Now, let us focus on identifying $k$ $[\epsilon, \rho]$-optimal arms.
In Theorem~\ref{thm:impossibility_qpk} we derive the lower bound on the sample complexity to solve an 
instance \QPK by reducing it to solving a \SUBSET problem as follows.

\begin{restatable}{theorem}{thmimpossibility_qpk}[Lower Bound on the Sample Complexity for Solving \protect\QPK]
\label{thm:impossibility_qpk}
% For $\alpha \in [0, 1)$, and $k > 1$,
For every $\epsilon \in (0, \frac{1}{\sqrt{32}}]$,
$\delta \in (0, \frac{1}{\sqrt{32}}]$,
and $\rho \in (0,\frac{1}{2}]$,
there exists an instance of \QPK given by $(\A, P_\A, \rho, \epsilon, \delta)$,
such that
any algorithm that solves \QPK incurs at least
%there exists no algorithm that solves it %that can solve every instance of \QPK given by $(\A, P_\A, \rho, \epsilon, \delta)$
%within a number of samples which is strictly lesser than 
$C\cdot \frac{k}{\rho\epsilon^2}\ln\frac{k}{8\delta}$ samples, where 
$C = \frac{1}{18375}$.
\end{restatable}
%\filler{THEOREM NEEDS TO BE WRITTEN BY SPECIFYING RANGES FOR EPSILON, DELTA, AND RHO.}
\begin{proof}
We shall prove the theorem by contradiction. Let us assume that  the
statement is incorrect. Therefore, there exists an algorithm \textsc{ALG} that \textsc{ALG} can solve
any instance of \QPK using no more than
$C\cdot \frac{k}{\rho\epsilon^2}\ln\frac{k}{8\delta}$ samples, for
$C= \frac{1}{18375}$. Now, let $(n, \A, m, \epsilon, \delta)$ be an instance of \SUBSET~\citep{bib:arcsk2017} with
$n \geq 2m$. Letting $P_\A = Uniform\{1,2, \dots, n\}$, $k = m$, and $\rho = m /n$, we
create an instance of \QPK as $(\A, P_\A, \rho, k, \epsilon, \delta)$. Therefore, solving
this \QPK instance will solve the original \SUBSET instance.
%Assuming $N$ as the number of samples needed by \textsc{ALG} to solve this \QPK instance,  we can write $N \leq C\cdot \frac{k}{\rho\epsilon^2}\ln\frac{k}{4\delta}$. Hence, a
According our claim, \textsc{ALG} solves the original \SUBSET instance using at most
$C\cdot \frac{k}{(k/n)\epsilon^2}\ln\frac{k}{8\delta}$
$ = C\cdot \frac{m}{(m/n)\epsilon^2}\ln\frac{m}{8\delta}$
$ = C\cdot \frac{n}{\epsilon^2}\ln\frac{m}{8\delta}$ samples. 
% Now,  for all $k > 1$, according to our assumption as $\alpha < 1$,
This observation contradicts the lower bound on the sample complexity for solving \SUBSET~\citep[Theorem 8]{bib:lucb}; thereby proving the theorem.
\end{proof}

% At this point it is of natural interest to find an efficient algorithm to solve \QPK. However, as discussed in Section~\ref{sec:problemdefinitionandcontributions}, problem instances have to be valid---that is, containing $k$ $[\epsilon, \rho]$-optimal arms. Even so, it can take an arbitrarily large number of guesses to discover unseen arms unless $P_\A$ allocates sufficient probability to each of $k$ arms. For this purpose, recall that we decided to limit our algorithms to at most $k$-equiprobable instances
% $(\A, P_\A, k, \rho, \epsilon, \delta)$ for which $\forall a \in \TOPRHO$, $\Pr_{\mathbf{a}' \sim P_\A}\{\mathbf{a}' = a\} \leq \frac{\rho}{k}$.

%\begin{definition}[At most equiprobable instance of \QPK:] Given a valid instance of \QPK as $(\A, P_\A, k, \rho, \epsilon, \delta)$, we call it \emph{at most equiprobable}, if $\forall a \in \TOPRHO$, $\Pr_{\mathbf{a}' \sim P_\A}\{\mathbf{a}' = a\} \leq \frac{\rho}{k}$.
%\end{definition}

\paragraph{Algorithm for solving at most $k$-equiprobable \protect\QPK instances.} Let, for any $\mathcal{S} \subseteq \A$, $\nu(\mathcal{S}) \defeq \Pr_{a \sim P_\A}\{a \in \mathcal{S}\}$. Therefore, $\nu(\A) = 1$.
Now, we present an algorithm \KQP that can solve any at most $k$-equiprobable instance of \QPK. 
Algorithm~\ref{alg:looseqpk} describes \KQP.
At each phase $y$, it solves an instance of \QP to output an arm, say $a^{(y)}$, from $\TOPRHO(\epsilon)$. In
the next phase, it updates the bandit instance $\A^{y+1} = \A^{y}\setminus\{a^{(y)}\}$,
the sampling distribution 
$P_{\A^{y+1}} = \frac{1}{1-\nu\left(\A\setminus\A^{y+1}\right)} P_{\A^{y}}$, and the target quantile $\rho^{y+1} = \rho^y-\nu(a^{(y)})$. However, as we
are not given the explicit form of $P_\A$, we realise $P_{\A^{y+1}}$ by rejection-sampling---if
$a' \in \A\setminus\A^{y+1}$ is chosen by $P_{\A}$, we simply discard $a'$, and
continue to sample $P_\A$ one more time. Because $\nu(\{a^y\})$ is not known explicitly,
we rely on the fact that $\nu(\{a^y\}) \leq \rho/k$: it is for this reason we require the instance to be at most $ k$-equiprobable.
Therefore, in each phase $y \geq 1$, $\rho^y - \rho/k \leq \rho^{y+1} \leq \rho^y - \nu\{a^y\}$, and hence,
\KQP solves an instance of \QP given by 
$\left(\A^{y}, P_{\A^{y}}, \rho-{(y-1)\rho}/{k}, \epsilon, \delta\right)$.

% We notice, in each phase the algorithm \KQP 
% queries the oracle with the arm output by \PPP.
% % As \PPP can solve any instance of \QP, 
% Therefore, if the original \QPK instance is at most
% uniform, we can pass an oracle as an input to \KQP which will produce $\nu(\{a^{(y)}\}) = \rho/k$ 
% for any $y \in \{1, \cdots, k\}$.
% For such an instance this oracle will help us to establish the upper bound on the
% expected sample complexity.
% \begin{algorithm}
% \caption{\KQP: Algorithm to solve a valid \QPK}
% \label{alg:looseqpk}
% \DontPrintSemicolon
% \KwIn{$\A, P_\A, k, \rho, \epsilon, \delta$, and an oracle that gives $ \Pr_{a \sim P_\A}\{a \in \mathcal{S}$, for any $\mathcal{S} \subseteq \A$.}
% \KwOut{Set of $k$ distinct arms from $\TOPRHO(\epsilon)$.}
% $\A^1 = \A, \rho^1 = \rho$.\;
% \For{$y = 1,2,3,\cdots,k$}{
% 	Run \PPP to solve the \QP instance given by
%     $(\A^y, P_{\A^y}, \rho^y, \epsilon, \frac{\delta}{k})$, and let $a^{(y)}$ be the output.\;
%     $\A^{y+1} = \A^{y}\setminus \{a^{(y)}\}$\;
%     $P_{\A^{y+1}} = \frac{1}{1-\nu\left(\A\setminus\A^{y+1}\right)} P_{\A^{y}}$.\;
%     $\rho^{y+1} = \rho^y-\nu(\{a^{(y)}\})$.\;
% }
% \end{algorithm}

\begin{algorithm}
\begin{algorithmic}
\small{
\REQUIRE {$\A, P_\A, k, \rho, \epsilon, \delta$.}
\ENSURE {Set of $k$ distinct arms from $\TOPRHO(\epsilon)$.}
\STATE $\A^1 = \A, \rho^1 = \rho$.
\FOR {$y = 1,2,3,\cdots,k$}{
    \STATE Run \PPP to solve the \QP instance given by
    \STATE $(\A^y, P_{\A^y}, \rho^y, \epsilon, \frac{\delta}{k})$, and let $a^{(y)}$ be the output.
    \STATE $\A^{y+1} = \A^{y}\setminus \{a^{(y)}\}$.
%     \STATE $P_{\A^{y+1}} = \frac{1}{1-\nu\left(\A\setminus\A^{y+1}\right)} P_{\A^{y}}$.
    \STATE $\rho^{y+1} = \rho^y-({(y-1)\rho})/{k}$.
}\ENDFOR
}
\end{algorithmic}
\caption{\KQP: Algorithm to solve a at most k-equiprobable \QPK instances}
\label{alg:looseqpk}
\end{algorithm}

In Theorem~\ref{thm:exsceqp} we present an upper bound on the expected sample complexity of \KQP.% \filler{NOT ADDRESSED: In the line just above the algorithm, and also in the algorithm, you are (incorrectly) subtracting $\rho/k$ from the existing $\rho^{y}$. I think you should instead subtract $\nu(\A^{(y)})$.} \textcolor{blue}{not clear to me what you mean}
\begin{theorem}
\label{thm:exsceqp}
Given any at most $k$-equiprobable instance of \QPK with $k > 1$, 
%  and an oracle that produces $\nu(\{a^{(y)}\}) = \rho/k$
%  for all $y \in  \{1, 2, \cdots, k\}$, \
 \KQP solves the instance with expected 
sample-complexity upper bounded by $O\left(\frac{k}{\epsilon^2}\left(\frac{\log k}{\rho}\log\frac{k}{\delta} + \log^2\frac{k}{\delta}\right)\right)$.
\end{theorem}
\begin{proof}
We break the proof in two parts: upper-bounding the sample complexity, and proving correctness.

\textbf{Sample complexity:} In phase $y$, the sample complexity of \PPP is upper-bounded as
$\text{SC}(y) \leq \frac{C}{\rho^y \epsilon^2}\log\frac{k}{\delta}$, for some constant $C$.
Therefore, the sample complexity of \KQP is upper bounded as 
{\small
\begin{align*}
& \sum_{y=1}^k \text{SC}(y) \leq \sum_{y=1}^k \frac{C}{ \epsilon^2}\left(\frac{1}{\rho^y}\log\frac{k}{\delta} + \log^2\frac{k}{\delta}\right),\\
%  &= \left(\frac{1}{\rho} + \sum_{y=2}^k \frac{1}{\rho - \sum_{j=1}^{y-1} \nu(\{a^{(j)}\})}\right) \frac{C}{\epsilon^2}\ln\frac{k}{\delta} +\\
%  &\hspace{1cm}  \frac{kC}{\epsilon^2}\ln^2\frac{k}{\delta},\\
& \leq  \frac{C}{\epsilon^2}\left(\log\frac{k}{\delta} \sum_{y=1}^k \frac{1}{\rho - (y-1)\frac{\rho}{k}}+ k\log^2\frac{k}{\delta}\right),\\
& = \frac{Ck}{\epsilon^2} \left(\frac{1}{\rho} \log\frac{k}{\delta} \sum_{y=1}^k \frac{1}{k-y+1} + \log^2\frac{k}{\delta}\right),\\
& \leq \frac{C'k}{\epsilon^2}\left(\frac{\log k}{\rho}\log\frac{k}{\delta} + \log^2\frac{k}{\delta}\right),
\end{align*}
}
for $k > 1$, and some constant $C'$.
\paragraph{Correctness:} 
Letting $E_y$ be the event that $a^{(y)} \not\in \TOPRHO(\epsilon)$, the probability of mistake by \KQP can be upper bounded as $\Pr\{\text{Error}\} = \Pr\{\exists y \in \{1, \cdots, k\}\; E_y \} \leq \sum_{y=1}^k \Pr\{E_y \} \leq \sum_{y=1}^k \frac{\delta}{k} = \delta$.
\end{proof}

\begin{corollary}
\label{cor:qfkfromqpktighter}
\KQP can solve any instance of \QFK given by $(\A, n, m, k, \epsilon, \delta)$ with $k \geq 2$, using $O\left(\frac{k}{\epsilon^2}\left(\frac{n\log k}{m}\log\frac{k}{\delta} + \log^2\frac{k}{\delta}\right)\right)$ samples.
\end{corollary}

We note that though the sample complexity of \KQP is independent of size of the bandit instance $\A$, and every
instance of \QFK given by $(\A, n, m, m , \epsilon, \delta)$, can be solved by \KQP by posing it as an instance of 
\QPK given by $(\A,  Uniform\{\A\}, m/n, m, \epsilon, \delta)$. However, for $k=m$, the sample complexity of \KQP
reduces to $O\left(\frac{1}{\epsilon^2}\left(n\log m\cdot\log\frac{m}{\delta} + \log^2\frac{m}{\delta}\right)\right)$,
which is higher than the sample complexity of \HALVING~\cite{bib:explorem}, that needs only  $O\left(\frac{n}{\epsilon^2}\log\frac{m}{\delta}\right)$ samples.
Hence, for the best subset selection problem 
in finite instances \HALVING is preferable to \KQP.
However, in the very large instances, where the probability
of picking any given arm from $\TOPRHO$ is close to zero,
% However,
% for potentially infinite instances, where the probability
% of picking any given arm is zero (or very close to zero),
\QPK is the ideal problem to solve, and \KQP is the first
solution that we propose.

\begin{corollary}
\label{cor:qfkfromqpktighter2}
Every instance of \QPK given by $(\A, P_\A, k, \rho, \epsilon, \delta)$, such that
$|\A| = \infty$, %and $\Pr_{a\sim P_\A}\{a \in S \subset \A : |S| < \infty\} = 0$,
and for all finite subset $S \subset \A$,  $\Pr_{a\sim P_\A}\{a \in S\} = 0$;
can be solved within a sample-complexity $O\left({k}{\epsilon^{-2}}\left({\rho^{-1}}\log({k}/{\delta}) + \log^2({k}/{\delta})\right)\right)$, by independently solving $k$ different \QP instances, each given by $(\A, P_\A, k, \rho, \epsilon, \delta/k)$.
\end{corollary}
The correctness of Corollary~\ref{cor:qfkfromqpktighter2} gets proved by noticing the fact that all the $k$ outputs are unique with probability 1, and then taking union bound over mistake probabilities.
% As the probability of encountering any arm more than once is zero, the each of the $k$ outputs is
% is unique with probability 1. Now, taking union over mistake probabilities, the statement of Corollary~\ref{cor:qfkfromqpktighter2} follows.
%
%\filler{Still not clear. Are you assuming $k = m$? Perhaps you need to specify the constraints on $k$ and $m$. Otherwise, look at the preceding sentence alone, which implies $(1, m, n)$ is better solved using HALVING unless $n = \infty$. Not true; what if $n$ is just large?}
Before going to the experiments, we present an important result on the hardness of solving \QP.
% Specifically,
% we establish a connection between the optimal sample complexities for solving \QF and \QP.
% \filler{Tight lower bound or tight upper bound?}

\subsection{On the Hardness of Solving \protect\QP}
\label{subsec:qpreducub}
% \filler{What do you mean by ``order-optimal''? The term suggests that you believe that the lower bound we have furnished is correct. But what if the actual lower bound is larger?}

Theorem~\ref{thm:qpreducubonly} presents a general relation between the upper bound on 
sample complexities for solving \QF and \QP.
 
% solving \QP problem using order-optimal sample complexity is crucially dependent on the
% existence of an algorithm for solving \QF within order-optimal sample complexity.
% The lower bound directly follows from the intuition. We provide the upper bound in Theorem~\ref{thm:qpreducub}.
% \filler{Not clear: lower bound for what and upper bound for what?}

% if every problem instance of
% \QP given by $(\A, P_\A, \rho, \epsilon, \delta)$   needs $\Theta(h(\rho,\epsilon,\delta))$ 
% samples to get solved.
%
% \subsection{Upper Bound}
% \label{subsec:hardnessqpub}

% In this section we show that if there exists an algorithm for solving any instance of \QF
% within order-optimal sample-complexity, then we can construct an algorithm that can solve
% any instance of \QP  within order-optimal sample-complexity.

\begin{restatable}{theorem}{thmqpreducubonly}
\label{thm:qpreducubonly}
Let $\gamma: \mathbb{Z}^+ \times \mathbb{Z}^+ \times [0,1] \times [0,1] \mapsto \mathbb{R}^+$.
If every instance of \QF  given by $(\A, n, m, \epsilon, \delta)$, can be solved
within the sample-complexity $O\left(\frac{n}{m\epsilon^2}\log\frac{1}{\delta} + \gamma(n,m,\epsilon,\delta)\right)$, 
then,
% there exists an algorithm \textsc{OptQP} that can solve 
every instance of \QP  given by $(\A, P_\A, \rho, \epsilon, \delta)$ can be solved
within the sample-complexity  
\resizebox{\columnwidth}{!}{$O\left({(1/\rho\epsilon^{2}})\log({1}/{\delta}) + \gamma\left(\ceil{8\log({2}/{\delta})}, \floor{4\log({2}/{\delta)}}, {\epsilon}/{2}, {\delta}/{2}\right)\right)$.}
\end{restatable}


We assume that there exists an algorithm \textsc{OptQF} that solves
every instance of \QF  given by $(\A, n, m, \epsilon, \delta)$,
using $O\left(\frac{n}{m\epsilon^2}\log\frac{1}{\delta} + \gamma(n,m,\epsilon,\delta)\right)$ samples.
% We prove Theorem~\ref{thm:qpreducub}
We establish the upper bound on sample complexity for solving \QP by constructing an  algorithm \textsc{OptQP}
that follows an approach similar to \PPP. Specifically, \textsc{OptQP} reduces the input \QP instance 
to an instance of \QF 
using $O\left(\frac{1}{\rho\epsilon^2}\log\frac{1}{\delta}\right)$ samples. Then, it solves that 
\QF  using \textsc{OptQF} as the subroutine. The detailed proof is given in Appendix-C.

\begin{corollary}
\label{cor:effectoptqf}
Corollary~\ref{cor:qffromqptighter} shows that every \QF is solvable in $O\left(\frac{1}{\epsilon^2}\left(\frac{n}{m}\log\frac{1}{\delta} + \log^2\frac{1}{\delta}\right)\right)$ samples.
Hence, $\gamma(n,m,\epsilon,\delta) \in O\left(\frac{1}{\epsilon^2}\log^2\frac{1}{\delta}\right)$,
and therefore, every \QP is solvable in $O\left(\frac{1}{\epsilon^2}\left(\frac{1}{\rho}\log\frac{1}{\delta} + \log^2\frac{1}{\delta}\right)\right)$ samples.

On the other hand, if the lower bound for solving \QF provided by \citet{bib:arcsk2017} matches the upper bound up to a constant factor, then $\gamma(n,m,\epsilon,\delta) \in \Theta\left(\frac{n}{m\epsilon^2}\log\frac{1}{\delta}\right)$. In that case, \QP is solvable using $\Theta\left(\frac{1}{\rho\epsilon^2}\log\frac{1}{\delta}\right)$ samples.
\end{corollary}


It is interesting to find a $\gamma(\cdot)$ such that the upper bound presented in Theorem~\ref{thm:qpreducubonly}
matches the lower bound up to a constant factor. We notice, Theorem~\ref{thm:qpreducubonly} guarantees that there exists a constant $C$,
such that for any given $\epsilon, \delta$, and $m \leq n/2$,
%for every $n \geq \ceil{8\log(2/\delta)}$, %and for every $m \leq n/2$,
$\gamma(n ,m, \epsilon, \delta) \leq C \cdot \gamma\left(\ceil{8\log(2/\delta)}, \floor{4\log(2/\delta)}, \frac{\epsilon}{2}, \frac{\delta}{2}\right)$. However, for $n < \ceil{8\log(2/\delta)}$ %and $m \leq n/2$,
we believe \QF can be solved more efficiently than posing it as \QP. We present it as a conjecture.

\begin{definition}
For $g: \mathbb{Z}^+ \times \mathbb{Z}^+ \times [0,1] \times [0,1] \mapsto \mathbb{R}^+$ we say \QF is solvable in $\Theta(g(\cdot))$, if there exists an algorithm that
solves every instance of \QF given by $(\A, n, m, \epsilon, \delta)$ in $O(g(n,m,\epsilon,\delta))$ samples,
and there exists an instance of \QF given by $(\bar{\A}, \bar{n}, \bar{m}, \bar{\epsilon}, \bar{\delta})$ such that every algorithm
needs at least $\Omega(g(\bar{n},\bar{m},\bar{\epsilon},\bar{\delta}))$ samples to solve it.
\end{definition}

\begin{restatable}{conjecture}{conjdiffqfqp}
There exists a constant $C > 0$, and functions $g: \mathbb{Z}^+ \times \mathbb{Z}^+ \times [0,1] \times [0,1] \mapsto \mathbb{R}^+$, and $h: \mathbb{Z}^+ \times \mathbb{Z}^+ \times [0,1] \times [0,1] \mapsto \mathbb{R}^+$,
such that for every $\delta \in (0, 1]$, there exists an integer $n_0 <  C\log\frac{2}{\delta}$, such that for
every $n\leq n_0$, \QF 
%given by $(\A, n, m, \epsilon, \delta)$ 
is solvable in $\Theta(g(n, m, \epsilon, \delta))$ samples, and its  equivalent \QP (obtained by posing the
the instance of \QF as an instance of \QP, as done in proving Corollary~\ref{cor:qffromqptighter}) needs at least  $\Omega(h(n, m, \epsilon, \delta))$ samples, then $\lim_{\delta \downarrow 0} \frac{g(n, m, \epsilon, \delta)}{h(n, m, \epsilon, \delta)} \to 0$.
\end{restatable}


% Analogously, for a function $h: [0,1] \times [0,1] \times [0,1] \mapsto \mathbb{R}^+$ we say \QP is solvable 
% in $\Theta(h(\cdot))$, if there exists an algorithm \textsc{OptQP} that
% solves every instance of \QP given by $(\A, P_\A, \rho, \epsilon, \delta)$ in $O(h(\rho,\epsilon,\delta))$ samples, and there exists an instance of \QP given by $(\A', P_{\A'}', \rho', \epsilon', \delta')$ such that every algorithm needs at least $\Omega(h(\rho',\epsilon',\delta'))$ samples to solve it.

 
% \begin{corollary}
% \label{cor:effectoptqf}
% If the lower bound provided by \citet{bib:arcsk2017} is tight up to a constant
% factor, then $\gamma(n,m,\epsilon,\delta) \in \Omega\left(\frac{n}{m\epsilon^2}\log\frac{1}{\delta}\right)$. In that case, \QP is solvable using $\Theta\left(\frac{1}{\rho\epsilon^2}\log\frac{1}{\delta}\right)$ samples.
% On the other hand, if \QF is solvable in $\Theta\left(\frac{1}{\epsilon^2}\left(\frac{n}{m}\log\frac{1}{\delta} + \log^2\frac{1}{\delta}\right)\right)$ samples, then 
% $\gamma(n,m,\epsilon,\delta) \in \Omega\left(\frac{1}{\epsilon^2}\log^2\frac{1}{\delta}\right)$,
% and hence, solving \QP is solvable in $\Theta\left(\frac{1}{\epsilon^2}\left(\frac{1}{\rho}\log\frac{1}{\delta} + \log^2\frac{1}{\delta}\right)\right)$ samples.
% \end{corollary}

% \filler{``If the lower bound provided by \citet{bib:arcsk2017} is tight up to a constant
% factor, then $\gamma(n,m,\epsilon,\delta) = C\frac{n}{m\epsilon^2}\log\frac{1}{\delta}$,
% for some constant $C$.'' Do you mean $\gamma = \Omega(whatever)$ or $\geq C \cdot whatever$?}

Next, we empirically compare \GLUCB for 
$k=1$ with \FF on different instances, and also we study empirical performance of \GLUCB by varying $k$.
