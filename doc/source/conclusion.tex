\section{Conclusion}
\label{sec:conclusion}
Identifying one arm out of the best $m$, in an $n$-armed stochastic bandit
is an interesting problem identified by \citet{bib:arcsk2017}. They have
mentioned the scenarios where identifying the best subset is practically infeasible.
However, there are numerous examples in practice that demand efficient identification
of multiple good solutions instead of only one; for example, assigning a distributed crowd-sourcing task, identification of good molecular combinations in drug designing, \etc. 
In this paper, we present \QFK---a generalised problem of identifying $k$ out of the best $m$ arms. Setting $k=1$, \QFK gets reduced to selection of one out of the best $m$ arms, while
setting $k=m$, makes it identical with the ``subset-selection''~\cite{bib:explorem}. We
have presented a lower bound on the sample complexity
to solve \QFK. 
% Besides, being motivated by \HALVING~\cite{bib:explorem},
% we have presented an algorithm \GHALVING that solves \QFK with worst case sample complexity
% within a constant factor of the lower bound. 
We have also presented a fully sequential adaptive PAC algorithm, \GLUCB,
that solves \QFK, with expected sample complexity matching up to a constant factor that of 
$\F_2$~\cite{bib:arcsk2017} and \LUCB~\cite{bib:lucb} for $k=1$ and $k=m$, respectively. We have empirically
compared \GLUCB to $\F_2$ on different problem instances, and
shown that \GLUCB outperforms $\F_2$ by a large margin in terms of the number of
samples as $n$ grows.

For the problem of identification of a single $[\epsilon, \rho]$-optimal~\cite{bib:arcsk2017}
arm in infinite bandit instances, the existing upper bound on the sample complexity
differs from the lower bound by a multiplicative $\log\frac{1}{\delta}$ factor. 
It was not clear whether the lower bound was loose, or the upper can be improved,
and left as an interesting problem to solve~\cite{Aziz+AKA:2018}. 
% In this paper, we bridge the gap by proposing a non-adaptive algorithm $\mathcal{P}_3$, thus eliminating the extra $\log\frac{1}{\delta}$ factor from the upper bound. 
In this paper we reduce the gap by furnishing an upper bound which is optimal up to an \textit{additive} poly-log term.
Further, we show that the problem of identification k distinct $[\epsilon, \rho]$-optimal 
arms is not well-posed in general, but when it is, we derive a lower bound on the sample complexity. 
Also, we identify a class of well-posed instances for which we present an efficient algorithm. 
In the end we show that how improving the upper bound on the sample-complexity for solving 
\QF instances can be translated in improving  upper bound on the 
sample-complexity for solving \QP. However, we conjecture that there exists a set of \QF instances 
and a corresponding set of \QP instances, such that every instance of \QF requires lesser number of samples to solve
than the corresponding \QP instance in the other set. Showing correctness of the conjecture and improving the
lower and the upper bound on the sample complexities are some interesting directions we leave for future work.
% In the end
% we show that existence of an algorithm that solves \QF within a order-optimal sample complexity ensures 
% the existence of algorithm that solves \QP within a order-optimal sample complexity. 
% Finding such an algorithm is a very interesting problem that we leave as a future work. 

